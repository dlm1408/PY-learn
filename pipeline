# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from joblib import dump, load

# Step 1: Extract and Load Data
data = pd.read_csv('data.csv')

# Step 2: Data Preprocessing (Transformation)
X = data.drop('target_column', axis=1)
y = data['target_column']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Define Preprocessing Steps
# For numeric features: impute missing values with mean and scale them
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

# For categorical features: impute missing values with most frequent and one-hot encode them
categorical_features = X.select_dtypes(include=['object']).columns
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Step 4: Define Pipeline with Preprocessing and Logistic Regression Model
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression())])

# Step 5: Train the Pipeline
pipeline.fit(X_train, y_train)

# Step 6: Predictions and Evaluation
y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Step 7: Save the Pipeline
dump(pipeline, 'logistic_regression_pipeline.joblib')

# Step 8: Load the Pipeline (optional)
loaded_pipeline = load('logistic_regression_pipeline.joblib')

#---------------------
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_auc_score
from joblib import dump, load

# Step 1: Extract and Load Data
data = pd.read_csv('data.csv')

# Step 2: Data Preprocessing (Transformation)
X = data.drop('target_column', axis=1)
y = data['target_column']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Define Preprocessing Steps
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

categorical_features = X.select_dtypes(include=['object']).columns
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Step 4: Define Pipelines for Multiple Algorithms
pipelines = {
    'Logistic Regression': Pipeline(steps=[('preprocessor', preprocessor), ('classifier', LogisticRegression())]),
    'Random Forest': Pipeline(steps=[('preprocessor', preprocessor), ('classifier', RandomForestClassifier())]),
    'Gradient Boosting': Pipeline(steps=[('preprocessor', preprocessor), ('classifier', GradientBoostingClassifier())]),
    'Support Vector Machine': Pipeline(steps=[('preprocessor', preprocessor), ('classifier', SVC(probability=True))]),
    'Decision Tree': Pipeline(steps=[('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier())])
}

# Step 5: Train Pipelines and Print AUC Scores
auc_scores = {}
for name, pipeline in pipelines.items():
    pipeline.fit(X_train, y_train)
    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]  # Probability of positive class
    auc = roc_auc_score(y_test, y_pred_proba)
    auc_scores[name] = auc
    print(f"AUC for {name}: {auc}")

# Step 6: Select Best Algorithm
best_algorithm = max(auc_scores, key=auc_scores.get)
print(f"Best Algorithm: {best_algorithm}")

# Step 7: Save the Best Pipeline
best_pipeline = pipelines[best_algorithm]
dump(best_pipeline, 'best_classification_pipeline.joblib')

# Now you can use 'best_classification_pipeline' as the final model for making predictions on new data

## In this updated version:

#Step 5 calculates the AUC scores for each algorithm and stores them in a dictionary.
#Step 6 selects the algorithm with the highest AUC score as the best algorithm.
#Step 7 saves the pipeline corresponding to the best algorithm as the final model for future use.

# Now you can use 'loaded_pipeline' to make predictions on new data
# --------------------------------------------#
## To print the original feature numbers, clustering results, and the best features selected by the feature selection methods, you can modify the code as follows:
# Step 3: Feature Clustering
kmeans = KMeans(n_clusters=5, random_state=42)
X_clustered = kmeans.fit_transform(X)

# Print original feature numbers
print("Original Feature Numbers:", len(X.columns))

# Print clustering result
print("Clustering Result:", kmeans.labels_)

# Step 4: Define Preprocessing Steps
# Include clustered features along with original features
clustered_features = [f'Cluster_{i}' for i in range(X_clustered.shape[1])]
numeric_features_with_cluster = list(numeric_features) + clustered_features

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features_with_cluster),
        ('cat', categorical_transformer, categorical_features)])

# Step 5: Define Pipelines for Multiple Algorithms with Feature Selection
pipelines = {
    'Logistic Regression with SelectKBest': Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('feature_selection', SelectKBest(f_classif, k=15)),
        ('classifier', LogisticRegression())]),

    'Random Forest with RFE': Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('feature_selection', RFE(estimator=RandomForestClassifier(), n_features_to_select=15)),
        ('classifier', RandomForestClassifier())])
}

# Step 6: Train Pipelines and Print AUC Scores
auc_scores = {}
for name, pipeline in pipelines.items():
    pipeline.fit(X_train, y_train)
    X_test_transformed = preprocessor.transform(X_test)
    y_pred_proba = pipeline.predict_proba(X_test_transformed)[:, 1]  # Probability of positive class
    auc = roc_auc_score(y_test, y_pred_proba)
    auc_scores[name] = auc
    print(f"AUC for {name}: {auc}")

# Step 7: Select Best Pipeline
best_algorithm = max(auc_scores, key=auc_scores.get)
best_pipeline = pipelines[best_algorithm]
print(f"Best Algorithm: {best_algorithm}")

# Step 8: Get Selected Features from the Best Pipeline
selected_features = best_pipeline.named_steps['feature_selection'].get_support(indices=True)
print("Best Features Selected:", selected_features)

# Step 9: Save the Best Pipeline
dump(best_pipeline, 'best_classification_pipeline_with_feature_selection_and_clustering.joblib')

# Now you can use 'best_classification_pipeline_with_feature_selection_and_clustering' as the final model for making predictions on new data
###----------------------------------------------##
# We print the original number of features before clustering.
# We print the clustering result.
# When defining preprocessing steps in Step 4, we include clustered features along with the original features.
# After training the pipelines, we extract the selected features from the best pipeline using get_support(indices=True) and print them.
# The best pipeline is then saved for future use.

###----------------------------------------------##

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.cluster import KMeans
from sklearn.metrics import roc_auc_score
from joblib import dump, load

# Step 1: Extract and Load Data
data = pd.read_csv('data.csv')

# Step 2: Data Preprocessing (Transformation)
X = data.drop('target_column', axis=1)
y = data['target_column']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Feature Clustering
kmeans = KMeans(n_clusters=5, random_state=42)
X_clustered = kmeans.fit_transform(X)

# Step 4: Define Preprocessing Steps
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

categorical_features = X.select_dtypes(include=['object']).columns
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Step 5: Define Pipelines for Multiple Algorithms with Feature Selection
pipelines = {
    'Logistic Regression with SelectKBest': Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('feature_selection', SelectKBest(f_classif, k=15)),
        ('classifier', LogisticRegression())]),

    'Random Forest with RFE': Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('feature_selection', RFE(estimator=RandomForestClassifier(), n_features_to_select=15)),
        ('classifier', RandomForestClassifier())])
}

# Step 6: Train Pipelines and Print AUC Scores
auc_scores = {}
for name, pipeline in pipelines.items():
    pipeline.fit(X_train, y_train)
    X_test_transformed = preprocessor.transform(X_test)
    y_pred_proba = pipeline.predict_proba(X_test_transformed)[:, 1]  # Probability of positive class
    auc = roc_auc_score(y_test, y_pred_proba)
    auc_scores[name] = auc
    print(f"AUC for {name}: {auc}")

# Step 7: Select Best Pipeline
best_algorithm = max(auc_scores, key=auc_scores.get)
best_pipeline = pipelines[best_algorithm]
print(f"Best Algorithm: {best_algorithm}")

# Step 8: Save the Best Pipeline
dump(best_pipeline, 'best_classification_pipeline_with_feature_selection_and_clustering.joblib')

# Now you can use 'best_classification_pipeline_with_feature_selection_and_clustering' as the final model for making predictions on new data

##-------------------------------##
In this updated version:

# Step 3 involves performing feature clustering using KMeans clustering with 5 clusters.
# Step 5 defines pipelines with feature selection after preprocessing. The preprocessing step includes feature clustering.
# The AUC scores for each pipeline are calculated and stored in a dictionary.
# The best pipeline (algorithm) is selected based on the highest AUC score.
# The selected best pipeline is then saved for future use.

#------ plot calibrition curve and check the fairness------#
"""To plot the calibration curve for each selected feature on the test data, we first need to obtain the predicted probabilities from the best pipeline for each feature individually. 
Then, we can use these probabilities to plot the calibration curves. Here's how you can do it:"""
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve

# Step 10: Plot Calibration Curve for Each Selected Feature
plt.figure(figsize=(10, 10))

for feature_idx in selected_features:
    # Select the feature from the test data
    X_test_feature = X_test.iloc[:, feature_idx:feature_idx+1]
    
    # Transform the feature using the preprocessing pipeline
    X_test_feature_transformed = preprocessor.transform(X_test_feature)
    
    # Predict probabilities using the best pipeline
    y_pred_proba = best_pipeline.predict_proba(X_test_feature_transformed)[:, 1]
    
    # Calculate calibration curve
    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_pred_proba, n_bins=10)
    
    # Plot calibration curve
    plt.plot(mean_predicted_value, fraction_of_positives, marker='o', label=f'Feature {feature_idx}')

plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Perfectly calibrated')
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.title('Calibration Curve for Selected Features')
plt.legend()
plt.show()
"""
We iterate over each selected feature.
For each feature, we select it from the test data.
We transform the selected feature using the preprocessing pipeline.
We predict probabilities using the best pipeline.
We calculate the calibration curve using calibration_curve.
We plot the calibration curve for each feature.
We also plot a dashed line representing a perfectly calibrated model.
"""


"""--------------------shap explain the model ---------------------------------
To explain the model using SHAP in decile level, we can follow these steps:

Predict probabilities for the test data using the best pipeline.
Divide the test data into deciles based on predicted probabilities.
Calculate SHAP values for each decile.
Aggregate SHAP values for each feature within each decile.
Visualize the aggregated SHAP values for each feature across deciles.
In this code:

We use SHAP to calculate the SHAP values for each feature in each decile.
We aggregate SHAP values across samples within each decile.
We visualize the aggregated SHAP values across deciles for each feature.

-----------------------------comment end----------------------------------"""
import numpy as np
import shap

# Step 10: Predict Probabilities for the Test Data
y_pred_proba_test = best_pipeline.predict_proba(X_test_transformed)[:, 1]

# Step 11: Divide the Test Data into Deciles
deciles = pd.qcut(y_pred_proba_test, q=10, labels=False)

# Step 12: Calculate SHAP Values for each Decile
shap_values_deciles = []

for decile in range(10):
    # Select data for the current decile
    X_decile = X_test_transformed[deciles == decile]
    if len(X_decile) == 0:
        shap_values_deciles.append(None)
        continue
    
    # Calculate SHAP values
    explainer = shap.Explainer(best_pipeline.named_steps['classifier'], X_train_transformed)
    shap_values = explainer.shap_values(X_decile)
    shap_values_deciles.append(shap_values)

# Step 13: Aggregate SHAP Values for Each Feature within Each Decile
shap_values_deciles_aggregated = []

for decile in range(10):
    if shap_values_deciles[decile] is None:
        shap_values_deciles_aggregated.append(None)
        continue
    
    # Aggregate SHAP values for each feature within the decile
    shap_values_deciles_aggregated.append(np.mean(np.abs(shap_values_deciles[decile]), axis=0))

# Step 14: Visualize the Aggregated SHAP Values for Each Feature across Deciles
plt.figure(figsize=(12, 8))

for feature_idx, feature_name in enumerate(X.columns):
    shap_values_feature = [shap_values_deciles_aggregated[decile][feature_idx] 
                           if shap_values_deciles_aggregated[decile] is not None else None
                           for decile in range(10)]
    
    plt.plot(range(1, 11), shap_values_feature, marker='o', label=f'Feature {feature_name}')

plt.xlabel('Decile')
plt.ylabel('Mean Absolute SHAP Value')
plt.title('Aggregated SHAP Values across Deciles for Each Feature')
plt.legend()
plt.xticks(range(1, 11))
plt.grid(True)


## -------------new code integrate all together ----------------------------
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, calibration_curve
from joblib import dump, load

# Step 1: Load Raw Data
data = pd.read_csv('data.csv')  # Assuming 'data.csv' contains raw data

# Step 2: Define Pipeline for Preprocessing
numeric_features = data.select_dtypes(include=['int64', 'float64']).columns
categorical_features = data.select_dtypes(include=['object']).columns

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Step 3: Define Pipeline for Feature Selection
clustered_features = []
for n_clusters in range(2, min(len(numeric_features), 10)):  # Change the range if needed
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(data[numeric_features])
    data[f'Cluster_{n_clusters}'] = kmeans.labels_
    clustered_features.extend([f'Cluster_{n_clusters}'])

all_features = list(numeric_features) + list(categorical_features) + clustered_features

feature_selector = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('feature_selection', SelectKBest(f_classif, k=12))])

# Step 4: Define Pipelines for Multiple Algorithms
algorithms = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC(probability=True),
    'Decision Tree': DecisionTreeClassifier()
}

final_model_pipeline = Pipeline(steps=[
    ('feature_selector', feature_selector),
    ('classifier', LogisticRegression())  # Default classifier, will be replaced later
])

# Step 5: Train Pipelines and Select Best Algorithm
X_train, X_test, y_train, y_test = train_test_split(data.drop('target_column', axis=1), data['target_column'], test_size=0.2, random_state=42)

best_auc = -1
best_algorithm = None

for name, algorithm in algorithms.items():
    final_model_pipeline.set_params(classifier=algorithm)
    final_model_pipeline.fit(X_train, y_train)
    y_pred_proba = final_model_pipeline.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"AUC for {name}: {auc}")
    if auc > best_auc:
        best_auc = auc
        best_algorithm = final_model_pipeline.named_steps['classifier']

print(f"Best Algorithm: {best_algorithm}")

# Step 6: Explain the Model with SHAP in Decile Level
y_pred_proba_test = final_model_pipeline.predict_proba(X_test)[:, 1]
deciles = pd.qcut(y_pred_proba_test, q=10, labels=False)

shap_values_deciles = []

for decile in range(10):
    X_decile = X_test[deciles == decile]
    if len(X_decile) == 0:
        shap_values_deciles.append(None)
        continue
    explainer = shap.Explainer(best_algorithm, preprocessor.transform(X_train))
    shap_values = explainer.shap_values(preprocessor.transform(X_decile))
    shap_values_deciles.append(shap_values)

# Step 7: Plot Calibration Curve for Selected Features
plt.figure(figsize=(12, 8))

selected_feature_indices = final_model_pipeline.named_steps['feature_selector'].named_steps['feature_selection'].get_support(indices=True)
selected_features = [all_features[i] for i in selected_feature_indices]

for feature_name in selected_features:
    X_test_feature = X_test[[feature_name]]
    X_test_feature_transformed = preprocessor.transform(X_test_feature)
    y_pred_proba = final_model_pipeline.predict_proba(X_test_feature_transformed)[:, 1]
    fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_pred_proba, n_bins=10)
    plt.plot(mean_predicted_value, fraction_of_positives, marker='o', label=f'{feature_name}')

plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Perfectly calibrated')
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.title('Calibration Curve for Selected Features')
plt.legend()
plt.grid(True)
plt.show()

plt.show()


